Text can be represented as a series of continuous words with length N using N-grams. The phrase "The quick brown fox jumps over the lazy dog" is an example of a sentence that may be expressed as a collection of 2-grams, often known as bigrams: "The quick", "quick brown", "brown fox", "fox jumps", "jumps over", "over the", "the lazy", and "lazy dog". N-grams allow us to model the probability of a sequence of words in a given language.

Language models, which are statistical models that assign probability to word sequences, are constructed using n-grams. We count the occurrences of each n-gram in a huge corpus of text in order to construct a language model using n-grams. We then use these counts to calculate the probabilities of the subsequent word given the preceding n-1 words.

N-grams can be used for a variety of things, such as text prediction, machine translation, and speech recognition. A speech recognition system, for instance, might utilize a language model to help discriminate between words with identical sounds. Similar to this, a machine translation system might make translation decisions using a language model.

It is easy to determine the probability for unigrams (single words) by counting the instances of each word in the corpus and dividing the result by the total number of words. For bigrams, we count the occurrences of each bigram and divide by the number of occurrences of the preceding word. For instance, the ratio of the number of times "quick brown fox" appears to the number of times "quick" appears would represent the probability of "brown fox" given "quick".

The source text is crucial in the process of creating a language model. The model will be more accurate the more representative the corpus is of the text it is being applied to. Also, the size of the corpus is important; larger corpora typically result in stronger models. The problem of unseen n-grams, where an n-gram exists in the test set but not in the training set, must be handled via smoothing. A straightforward method of smoothing is adding a tiny count (such as 1) to every n-gram, ensuring that even undiscovered n-grams have a non-zero chance.

By selecting from the probability distribution of the following word given the prior n-1 words, language models can be applied to the creation of text. To create longer text sequences, continue this process. The model is merely employing probabilities based on the prior words and does not have a deep knowledge of the content or context of the text, which has the potential to result in nonsensical or repetitious material. Metrics like perplexity, which assesses how well the model predicts a held-out set of test data, can be used to assess language models. Better performance is indicated by a lower confusion.

Users can look for and see the frequency of n-grams in Google's huge corpus of text data using the n-gram viewer tool. For instance, a search for "burger, pizza" reveals that "burger" is less frequent than "pizza" in the corpus. 
